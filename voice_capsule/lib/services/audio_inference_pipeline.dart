import 'dart:async';

import '../constants/settings_constants.dart';
import 'asr/asr_engine.dart';
import 'asr/asr_engine_factory.dart';
import 'asr/zipformer_engine.dart';
import 'audio_capture.dart';
import 'model_manager.dart';
import 'settings_service.dart';

/// æµæ°´çº¿çŠ¶æ€æšä¸¾
enum PipelineState {
  idle,
  initializing,
  running,
  stopping,
  error,
}

/// æµæ°´çº¿é”™è¯¯ç±»å‹
enum PipelineError {
  none,
  audioInitFailed,
  modelNotReady,
  recognizerFailed,
  deviceUnavailable,
}

/// å»¶è¿Ÿç»Ÿè®¡ä¿¡æ¯ (AC5: ç«¯åˆ°ç«¯å»¶è¿Ÿ < 200ms)
class LatencyStats {
  final int sampleCount;
  final double avgLatencyMs;
  final double maxLatencyMs;
  final int overThresholdCount;

  const LatencyStats({
    required this.sampleCount,
    required this.avgLatencyMs,
    required this.maxLatencyMs,
    required this.overThresholdCount,
  });

  factory LatencyStats.empty() => const LatencyStats(
        sampleCount: 0,
        avgLatencyMs: 0,
        maxLatencyMs: 0,
        overThresholdCount: 0,
      );

  @override
  String toString() =>
      'LatencyStats(samples: $sampleCount, avg: ${avgLatencyMs.toStringAsFixed(1)}ms, '
      'max: ${maxLatencyMs.toStringAsFixed(1)}ms, over200ms: $overThresholdCount)';
}

/// VAD ç«¯ç‚¹äº‹ä»¶ (Story 2-6)
class EndpointEvent {
  /// æœ€ç»ˆè¯†åˆ«æ–‡æœ¬
  final String finalText;

  /// æ˜¯å¦ç”± VAD è‡ªåŠ¨è§¦å‘ (true: VAD, false: æ‰‹åŠ¨ stop)
  final bool isVadTriggered;

  /// ç«¯ç‚¹è§¦å‘å‰çš„è¯†åˆ«æ—¶é•¿ (æ¯«ç§’)
  final int durationMs;

  /// å»¶è¿Ÿç»Ÿè®¡
  final LatencyStats latencyStats;

  /// Story 3-7: æ˜¯å¦ç”±è®¾å¤‡æ–­å¼€è§¦å‘
  final bool isDeviceLost;

  const EndpointEvent({
    required this.finalText,
    required this.isVadTriggered,
    required this.durationMs,
    required this.latencyStats,
    this.isDeviceLost = false,
  });

  @override
  String toString() =>
      'EndpointEvent(text: "$finalText", vad: $isVadTriggered, duration: ${durationMs}ms, deviceLost: $isDeviceLost)';
}

/// VAD é…ç½® (Story 2-6)
class VadConfig {
  /// æ˜¯å¦å¯ç”¨ VAD è‡ªåŠ¨åœæ­¢
  final bool autoStopOnEndpoint;

  /// ç«¯ç‚¹è§¦å‘åæ˜¯å¦è‡ªåŠ¨é‡ç½®æµçŠ¶æ€ (ç”¨äºè¿ç»­è¯†åˆ«)
  final bool autoReset;

  /// è‡ªå®šä¹‰ Rule 2 é™éŸ³é˜ˆå€¼ (ç§’)ï¼Œnull è¡¨ç¤ºä½¿ç”¨ SherpaConfig é»˜è®¤å€¼
  /// æ³¨æ„: æ­¤å€¼åœ¨ start() æ—¶ä¼ é€’ç»™ Sherpaï¼Œè¿è¡Œæ—¶ä¿®æ”¹éœ€é‡å¯ Pipeline
  final double? silenceThresholdSec;

  const VadConfig({
    this.autoStopOnEndpoint = true,
    this.autoReset = false,
    this.silenceThresholdSec,
  });

  /// é»˜è®¤é…ç½®: è‡ªåŠ¨åœæ­¢ï¼Œä¸è‡ªåŠ¨é‡ç½®
  factory VadConfig.defaultConfig() => const VadConfig();

  /// è¿ç»­è¯†åˆ«é…ç½®: ä¸åœæ­¢ï¼Œè‡ªåŠ¨é‡ç½®
  factory VadConfig.continuous() => const VadConfig(
        autoStopOnEndpoint: false,
        autoReset: true,
      );
}

/// éŸ³é¢‘-æ¨ç†æµæ°´çº¿
///
/// å°†éŸ³é¢‘é‡‡é›†å’Œè¯­éŸ³è¯†åˆ«æ•´åˆä¸ºä¸€ä½“çš„æµæ°´çº¿æœåŠ¡ã€‚
/// å®ç°é›¶æ‹·è´æ•°æ®æµ: PortAudio -> ASREngine (åŒä¸€å†…å­˜æŒ‡é’ˆ)
///
/// ä½¿ç”¨ç¤ºä¾‹:
/// ```dart
/// final pipeline = AudioInferencePipeline(
///   audioCapture: audioCapture,
///   asrEngine: ZipformerEngine(),
///   modelManager: modelManager,
/// );
///
/// pipeline.resultStream.listen((text) => print('è¯†åˆ«: $text'));
/// await pipeline.start();
/// // ... å½•éŸ³ä¸­ ...
/// final finalText = await pipeline.stop();
/// await pipeline.dispose();
/// ```
class AudioInferencePipeline {
  // === å¸¸é‡å®šä¹‰ ===
  /// é»˜è®¤ Rule2 é™éŸ³é˜ˆå€¼ (ç§’) - ä¸ ZipformerConfig é»˜è®¤å€¼ä¿æŒä¸€è‡´
  static const double kDefaultRule2Silence = 1.2;
  // === ä¾èµ–æ³¨å…¥ (é€šè¿‡æ„é€ å‡½æ•°ä¼ å…¥ï¼Œä¾¿äºæµ‹è¯•) ===
  final AudioCapture _audioCapture;
  ASREngine _asrEngine; // æ”¹ä¸º ASREngine æŠ½è±¡æ¥å£
  final ModelManager _modelManager;

  // === é…ç½®é€‰é¡¹ ===
  final bool enableDebugLog;

  // === çŠ¶æ€ç®¡ç† ===
  final StreamController<String> _resultController =
      StreamController.broadcast();
  final StreamController<PipelineState> _stateController =
      StreamController.broadcast();
  final StreamController<EndpointEvent> _endpointController =
      StreamController.broadcast(); // Story 2-6: VAD ç«¯ç‚¹äº‹ä»¶æµ
  PipelineState _state = PipelineState.idle;
  PipelineError _lastError = PipelineError.none;
  bool _stopRequested = false;
  String _lastEmittedText = ''; // ç”¨äºå»é‡
  Completer<void>? _loopCompleter; // è·Ÿè¸ªå¾ªç¯å®ŒæˆçŠ¶æ€
  bool _isDisposed = false; // M1 ä¿®å¤: é˜²æ­¢åœ¨å…³é—­åè®¿é—® StreamController

  // Story 2-6: VAD ç›¸å…³çŠ¶æ€
  VadConfig _vadConfig = VadConfig.defaultConfig();
  DateTime? _recordingStartTime;
  bool _vadTriggeredStop = false; // é˜²æ­¢ VAD å’Œ stop() å‘é€é‡å¤äº‹ä»¶

  // AC5 å»¶è¿Ÿæµ‹é‡ (H1 ä¿®å¤)
  static const int _latencyThresholdMs = 200;
  final List<double> _latencySamples = [];
  double _maxLatencyMs = 0;

  // === æ„é€ å‡½æ•° ===
  AudioInferencePipeline({
    required AudioCapture audioCapture,
    required ASREngine asrEngine,
    required ModelManager modelManager,
    this.enableDebugLog = false,
    VadConfig? vadConfig, // Story 2-6: å¯é€‰ VAD é…ç½®
  })  : _audioCapture = audioCapture,
        _asrEngine = asrEngine,
        _modelManager = modelManager {
    if (vadConfig != null) {
      _vadConfig = vadConfig;
    }
  }

  // === å…¬å¼€æ¥å£ ===

  /// è¯†åˆ«ç»“æœæµ (å»é‡åçš„æ–‡æœ¬)
  Stream<String> get resultStream => _resultController.stream;

  /// çŠ¶æ€å˜åŒ–æµ
  Stream<PipelineState> get stateStream => _stateController.stream;

  /// Story 2-6: VAD ç«¯ç‚¹äº‹ä»¶æµ
  Stream<EndpointEvent> get endpointStream => _endpointController.stream;

  /// æ˜¯å¦æ­£åœ¨è¿è¡Œ
  bool get isRunning => _state == PipelineState.running;

  /// å½“å‰çŠ¶æ€
  PipelineState get state => _state;

  /// æœ€è¿‘ä¸€æ¬¡é”™è¯¯
  PipelineError get lastError => _lastError;

  /// Story 3-7: è·å–æ›´è¯¦ç»†çš„ ASR é”™è¯¯ (AC9: æ˜¾ç¤ºå…·ä½“åŸå› )
  ASRError get lastASRError => _asrEngine.lastError;

  /// Story 2-6: å½“å‰ VAD é…ç½®
  VadConfig get vadConfig => _vadConfig;

  /// Story 2-6: è®¾ç½® VAD é…ç½® (ä»…åœ¨ idle çŠ¶æ€æœ‰æ•ˆ)
  ///
  /// è¿”å› true è¡¨ç¤ºè®¾ç½®æˆåŠŸï¼Œè¿”å› false è¡¨ç¤ºå½“å‰çŠ¶æ€ä¸å…è®¸ä¿®æ”¹ã€‚
  /// æ³¨æ„: å¦‚æœ Pipeline æ­£åœ¨è¿è¡Œï¼Œé…ç½®å°†ä¸ä¼šç”Ÿæ•ˆï¼Œéœ€è¦å…ˆè°ƒç”¨ stop()ã€‚
  bool setVadConfig(VadConfig config) {
    if (_state != PipelineState.idle) {
      if (enableDebugLog) {
        // ignore: avoid_print
        print('[Pipeline] âš ï¸ setVadConfig å¤±è´¥: å½“å‰çŠ¶æ€ä¸º $_stateï¼Œéœ€è¦ idle çŠ¶æ€');
      }
      return false;
    }
    _vadConfig = config;
    return true;
  }

  /// çƒ­åˆ‡æ¢æ¨¡å‹ç‰ˆæœ¬ (Zipformer int8/standard)
  ///
  /// æ³¨æ„ï¼šç”±äº onnxruntime çš„é™åˆ¶ï¼Œè¿è¡Œæ—¶åˆ‡æ¢æ¨¡å‹éœ€è¦é‡å¯åº”ç”¨ã€‚
  /// æ­¤æ–¹æ³•ä»…ä¿å­˜é…ç½®ï¼Œä¸è¿›è¡Œå®é™…åˆ‡æ¢ã€‚
  ///
  /// è¿”å› true è¡¨ç¤ºé…ç½®å·²ä¿å­˜ï¼ˆéœ€è¦é‡å¯ç”Ÿæ•ˆï¼‰ï¼Œfalse è¡¨ç¤ºå¤±è´¥
  Future<bool> switchModelType(ModelType modelType) async {
    // é…ç½®å·²åœ¨ SettingsService ä¸­ä¿å­˜ï¼Œè¿™é‡Œåªè¿”å›æˆåŠŸ
    // å®é™…åˆ‡æ¢ä¼šåœ¨ä¸‹æ¬¡åº”ç”¨å¯åŠ¨æ—¶ç”Ÿæ•ˆ
    return true;
  }

  /// Story 2-7: åˆ‡æ¢ ASR å¼•æ“ (Zipformer â†” SenseVoice)
  ///
  /// æ³¨æ„ï¼šç”±äº onnxruntime çš„é™åˆ¶ï¼Œåˆ‡æ¢å¼•æ“éœ€è¦é”€æ¯å¹¶é‡å»º Pipelineã€‚
  /// æ­¤æ“ä½œä¼šåœæ­¢å½“å‰å½•éŸ³ï¼Œé‡Šæ”¾æ—§å¼•æ“èµ„æºï¼Œç„¶ååˆ›å»ºæ–°å¼•æ“ã€‚
  ///
  /// [newEngine] æ–°çš„ ASR å¼•æ“å®ä¾‹
  ///
  /// è¿”å› [PipelineError.none] è¡¨ç¤ºæˆåŠŸ
  Future<PipelineError> switchEngine(ASREngine newEngine) async {
    // 1. å¦‚æœæ­£åœ¨è¿è¡Œï¼Œå…ˆåœæ­¢
    if (_state == PipelineState.running) {
      await stop();
    }

    // 2. é‡Šæ”¾æ—§å¼•æ“èµ„æº (ä½†ä¸å…³é—­ StreamController)
    _asrEngine.dispose();

    // 3. æ›¿æ¢ä¸ºæ–°å¼•æ“
    _asrEngine = newEngine;

    // 4. é‡ç½®çŠ¶æ€
    _lastError = PipelineError.none;
    _lastEmittedText = '';

    if (enableDebugLog) {
      // ignore: avoid_print
      print('[Pipeline] ğŸ”„ å¼•æ“å·²åˆ‡æ¢ä¸º: ${newEngine.engineType}');
    }

    return PipelineError.none;
  }

  /// Story 2-7: è·å–å½“å‰å¼•æ“ç±»å‹
  ASREngineType get currentEngineType => _asrEngine.engineType;

  /// è·å–å»¶è¿Ÿç»Ÿè®¡ä¿¡æ¯ (AC5: ç«¯åˆ°ç«¯å»¶è¿Ÿ < 200ms)
  LatencyStats get latencyStats {
    if (_latencySamples.isEmpty) {
      return LatencyStats.empty();
    }
    final sum = _latencySamples.reduce((a, b) => a + b);
    final overCount =
        _latencySamples.where((l) => l > _latencyThresholdMs).length;
    return LatencyStats(
      sampleCount: _latencySamples.length,
      avgLatencyMs: sum / _latencySamples.length,
      maxLatencyMs: _maxLatencyMs,
      overThresholdCount: overCount,
    );
  }

  /// å¯åŠ¨æµæ°´çº¿
  ///
  /// åˆå§‹åŒ–éŸ³é¢‘é‡‡é›†å’Œè¯†åˆ«å¼•æ“ï¼Œç„¶åå¼€å§‹é‡‡é›†å¾ªç¯ã€‚
  ///
  /// è¿”å›é”™è¯¯ç±»å‹:
  /// - [PipelineError.none] æˆåŠŸ
  /// - [PipelineError.modelNotReady] æ¨¡å‹æœªå°±ç»ª
  /// - [PipelineError.recognizerFailed] Sherpa åˆå§‹åŒ–å¤±è´¥
  /// - [PipelineError.audioInitFailed] éŸ³é¢‘è®¾å¤‡åˆå§‹åŒ–å¤±è´¥
  Future<PipelineError> start() async {
    if (_state == PipelineState.running) {
      return PipelineError.none;
    }

    _setState(PipelineState.initializing);

    // AC5: é‡ç½®å»¶è¿Ÿç»Ÿè®¡
    _latencySamples.clear();
    _maxLatencyMs = 0;

    // Story 2-6: é‡ç½® VAD çŠ¶æ€
    _recordingStartTime = DateTime.now();
    _vadTriggeredStop = false;

    // 1. æ£€æŸ¥å½“å‰å¼•æ“çš„æ¨¡å‹å°±ç»ªçŠ¶æ€
    final engineType = _asrEngine.engineType == ASREngineType.zipformer
        ? EngineType.zipformer
        : EngineType.sensevoice;
    if (!_modelManager.isEngineReady(engineType)) {
      _setError(PipelineError.modelNotReady);
      return _lastError;
    }

    // 2. åˆå§‹åŒ– ASREngine (ä½¿ç”¨ VadConfig ä¸­çš„é™éŸ³é˜ˆå€¼å’Œ SettingsService ä¸­çš„æ¨¡å‹ç±»å‹)
    final silenceThreshold =
        _vadConfig.silenceThresholdSec ?? kDefaultRule2Silence;
    final useInt8 = SettingsService.instance.isInitialized
        ? SettingsService.instance.modelType == ModelType.int8
        : true; // é»˜è®¤ä½¿ç”¨ int8

    // æ ¹æ®å¼•æ“ç±»å‹åˆ›å»ºé…ç½®
    final ASRConfig config;
    if (_asrEngine.engineType == ASREngineType.zipformer) {
      config = ZipformerConfig(
        modelDir: _modelManager.getModelPathForEngine(EngineType.zipformer),
        useInt8Model: useInt8,
        numThreads: 2,
        sampleRate: 16000,
        enableEndpoint: true, // Story 2-6: VAD ç«¯ç‚¹æ£€æµ‹
        rule1MinTrailingSilence: 2.4,
        rule2MinTrailingSilence: silenceThreshold,
        rule3MinUtteranceLength: 20.0,
      );
    } else {
      // SenseVoice é…ç½®
      config = SenseVoiceConfig(
        modelDir: _modelManager.getModelPathForEngine(EngineType.sensevoice),
        vadModelPath: _modelManager.vadModelFilePath,
      );
    }

    final asrError = await _asrEngine.initialize(config);
    if (asrError != ASRError.none) {
      _setError(PipelineError.recognizerFailed);
      return _lastError;
    }

    // 3. å¯åŠ¨ AudioCapture
    final audioError = await _audioCapture.start();
    if (audioError != AudioCaptureError.none) {
      _setError(PipelineError.audioInitFailed);
      return _lastError;
    }

    // 4. æ›´æ–°çŠ¶æ€å¹¶å¯åŠ¨é‡‡é›†å¾ªç¯
    _setState(PipelineState.running);
    _startCaptureLoop(); // ä¸ awaitï¼Œåå°è¿è¡Œ
    return PipelineError.none;
  }

  /// åœæ­¢æµæ°´çº¿
  ///
  /// åœæ­¢é‡‡é›†ï¼Œå¤„ç†å‰©ä½™æ•°æ®ï¼Œè¿”å›æœ€ç»ˆè¯†åˆ«ç»“æœã€‚
  Future<String> stop() async {
    if (_state != PipelineState.running) {
      return _lastEmittedText;
    }

    _setState(PipelineState.stopping);
    _stopRequested = true;

    // M3 ä¼˜åŒ–: ä½¿ç”¨æ›´çŸ­çš„è½®è¯¢é—´éš”ç­‰å¾…å¾ªç¯é€€å‡º
    if (_loopCompleter != null && !_loopCompleter!.isCompleted) {
      const maxWaitMs = 300;
      const pollIntervalMs = 20;
      var waitedMs = 0;
      while (!_loopCompleter!.isCompleted && waitedMs < maxWaitMs) {
        await Future.delayed(const Duration(milliseconds: pollIntervalMs));
        waitedMs += pollIntervalMs;
      }
    }

    // è·å–æœ€ç»ˆè¯†åˆ«ç»“æœ
    _asrEngine.inputFinished();
    while (_asrEngine.isReady()) {
      _asrEngine.decode();
    }
    final finalResult = _asrEngine.getResult();

    // Story 2-6: å‘é€æ‰‹åŠ¨åœæ­¢äº‹ä»¶ (ä»…å½“ä¸æ˜¯ VAD è§¦å‘æ—¶)
    if (!_vadTriggeredStop && !_isDisposed && !_endpointController.isClosed) {
      final durationMs = _recordingStartTime != null
          ? DateTime.now().difference(_recordingStartTime!).inMilliseconds
          : 0;
      final event = EndpointEvent(
        finalText: finalResult.text,
        isVadTriggered: false,
        durationMs: durationMs,
        latencyStats: latencyStats,
      );
      _endpointController.add(event);
    }

    // åœæ­¢éŸ³é¢‘é‡‡é›†
    await _audioCapture.stop();

    // é‡ç½® ASR æµçŠ¶æ€ (ä¿ç•™æ¨¡å‹ï¼Œåªæ¸…ç©ºç¼“å†²åŒº)
    _asrEngine.reset();

    // é‡ç½®çŠ¶æ€
    _stopRequested = false;
    _lastEmittedText = '';
    _loopCompleter = null;
    _vadTriggeredStop = false; // Story 2-6: é‡ç½®æ ‡å¿—
    _recordingStartTime = null; // Story 2-6: æ¸…ç©ºå½•éŸ³å¼€å§‹æ—¶é—´
    _setState(PipelineState.idle);

    return finalResult.text;
  }

  /// é‡Šæ”¾æ‰€æœ‰èµ„æº
  Future<void> dispose() async {
    // M1 ä¿®å¤: æ ‡è®°å·²é‡Šæ”¾ï¼Œé˜²æ­¢åç»­è®¿é—® StreamController
    _isDisposed = true;

    // 1. å¦‚æœæ­£åœ¨è¿è¡Œæˆ–æ­£åœ¨åœæ­¢ï¼Œå…ˆç¡®ä¿åœæ­¢
    if (_state == PipelineState.running || _state == PipelineState.stopping) {
      _stopRequested = true;
      // M3 ä¼˜åŒ–: ä½¿ç”¨æ›´çŸ­çš„è½®è¯¢é—´éš”
      if (_loopCompleter != null && !_loopCompleter!.isCompleted) {
        const maxWaitMs = 300;
        const pollIntervalMs = 20;
        var waitedMs = 0;
        while (!_loopCompleter!.isCompleted && waitedMs < maxWaitMs) {
          await Future.delayed(const Duration(milliseconds: pollIntervalMs));
          waitedMs += pollIntervalMs;
        }
      }
      await _audioCapture.stop();
    }

    // 2. å…³é—­ StreamController
    await _resultController.close();
    await _stateController.close();
    await _endpointController.close(); // Story 2-6: å…³é—­ç«¯ç‚¹äº‹ä»¶æµ

    // 3. é‡Šæ”¾åŸç”Ÿèµ„æº
    _audioCapture.dispose();
    _asrEngine.dispose();

    // 4. æ¸…ç†çŠ¶æ€
    _state = PipelineState.idle;
    _loopCompleter = null;
    _latencySamples.clear();
    _maxLatencyMs = 0;
  }

  // === ç§æœ‰æ–¹æ³• ===

  /// å¯åŠ¨é‡‡é›†-æ¨ç†å¾ªç¯
  Future<void> _startCaptureLoop() async {
    _loopCompleter = Completer<void>();
    final stopwatch = Stopwatch();
    const targetDurationMs = 100;

    while (!_stopRequested && _state == PipelineState.running) {
      stopwatch.reset();
      stopwatch.start();

      await _processSingleChunk();

      // æå‰æ£€æŸ¥åœæ­¢æ ‡å¿—
      if (_stopRequested || _state != PipelineState.running) break;

      stopwatch.stop();

      // æ€§èƒ½ç›‘æ§: è®°å½•è¶…è¿‡é˜ˆå€¼çš„æƒ…å†µ
      if (enableDebugLog && stopwatch.elapsedMilliseconds > 20) {
        // ignore: avoid_print
        print('[Pipeline] å¤„ç†è€—æ—¶: ${stopwatch.elapsedMilliseconds}ms');
      }

      // å¯ä¸­æ–­çš„å»¶è¿Ÿ: æ¯ 10ms æ£€æŸ¥ä¸€æ¬¡åœæ­¢æ ‡å¿—
      final elapsedMs = stopwatch.elapsedMilliseconds;
      if (elapsedMs < targetDurationMs) {
        final remainingMs = targetDurationMs - elapsedMs;
        await _interruptibleDelay(remainingMs);
      }
    }

    // å¾ªç¯ç»“æŸï¼Œæ ‡è®°å®Œæˆ
    if (!_loopCompleter!.isCompleted) {
      _loopCompleter!.complete();
    }

    // Story 2-6: VAD è‡ªåŠ¨åœæ­¢æ—¶çš„æ¸…ç†é€»è¾‘
    if (_vadTriggeredStop && _state == PipelineState.running) {
      await _audioCapture.stop();
      _asrEngine.reset();
      _stopRequested = false;
      _lastEmittedText = '';
      _vadTriggeredStop = false;
      _recordingStartTime = null;
      _setState(PipelineState.idle);
    }
  }

  /// å¯ä¸­æ–­çš„å»¶è¿Ÿï¼Œæ¯ 10ms æ£€æŸ¥ä¸€æ¬¡åœæ­¢æ ‡å¿—
  Future<void> _interruptibleDelay(int totalMs) async {
    const checkIntervalMs = 10;
    var remainingMs = totalMs;

    while (
        remainingMs > 0 && !_stopRequested && _state == PipelineState.running) {
      final delayMs =
          remainingMs > checkIntervalMs ? checkIntervalMs : remainingMs;
      await Future.delayed(Duration(milliseconds: delayMs));
      remainingMs -= delayMs;
    }
  }

  /// å¤„ç†å•ä¸ªéŸ³é¢‘å—
  Future<void> _processSingleChunk() async {
    // M1 ä¿®å¤: å¦‚æœå·²é‡Šæ”¾ï¼Œç›´æ¥è¿”å›
    if (_isDisposed) return;

    // AC5 å»¶è¿Ÿæµ‹é‡: è®°å½•éŸ³é¢‘é‡‡é›†å¼€å§‹æ—¶é—´
    final chunkStartTime = DateTime.now();

    // é›¶æ‹·è´: ç›´æ¥ä½¿ç”¨ AudioCapture çš„å†…éƒ¨ç¼“å†²åŒº
    final buffer = _audioCapture.buffer;
    final samplesRead = _audioCapture.read(buffer, AudioConfig.framesPerBuffer);

    // é”™è¯¯æ£€æŸ¥: read() è¿”å› -1 è¡¨ç¤ºé”™è¯¯
    if (samplesRead == -1) {
      final error = _audioCapture.lastReadError;
      if (error == AudioCaptureError.deviceUnavailable) {
        // Story 3-7: è®¾å¤‡ä¸¢å¤±æ—¶å‘é€ EndpointEvent å¹¶ä¿å­˜å·²è¯†åˆ«æ–‡æœ¬
        _handleDeviceLost();
        _setError(PipelineError.deviceUnavailable);
        _stopRequested = true; // è§¦å‘å¾ªç¯é€€å‡º
      }
      return;
    }

    if (samplesRead > 0) {
      // åŒä¸€æŒ‡é’ˆä¼ ç»™ ASREngine (é›¶æ‹·è´)
      _asrEngine.acceptWaveform(
          AudioConfig.sampleRate, buffer, samplesRead);

      // è§£ç å¹¶è·å–ç»“æœ (ä»…å½“æœ‰è¶³å¤Ÿæ•°æ®æ—¶)
      while (_asrEngine.isReady()) {
        _asrEngine.decode();
      }

      final result = _asrEngine.getResult();

      // å»é‡: åªåœ¨æ–‡æœ¬å˜åŒ–æ—¶å‘é€äº‹ä»¶
      if (result.text.isNotEmpty && result.text != _lastEmittedText) {
        // AC5 å»¶è¿Ÿæµ‹é‡: è®¡ç®—ç«¯åˆ°ç«¯å»¶è¿Ÿ (éŸ³é¢‘é‡‡é›†åˆ°ç»“æœè¾“å‡º)
        final latencyMs =
            DateTime.now().difference(chunkStartTime).inMilliseconds.toDouble();
        _latencySamples.add(latencyMs);
        if (latencyMs > _maxLatencyMs) {
          _maxLatencyMs = latencyMs;
        }
        if (enableDebugLog && latencyMs > _latencyThresholdMs) {
          // ignore: avoid_print
          print(
              '[Pipeline] âš ï¸ å»¶è¿Ÿè¶…æ ‡: ${latencyMs.toStringAsFixed(1)}ms > ${_latencyThresholdMs}ms');
        }

        _lastEmittedText = result.text;
        // M1 ä¿®å¤: æ£€æŸ¥ StreamController æ˜¯å¦å·²å…³é—­
        if (!_isDisposed && !_resultController.isClosed) {
          _resultController.add(result.text);
        }
      }

      // Story 2-6: VAD ç«¯ç‚¹æ£€æµ‹
      final isEndpointDetected = _asrEngine.isEndpoint();
      // è°ƒè¯•ï¼šåªåœ¨æœ‰è¯†åˆ«ç»“æœæˆ–ç«¯ç‚¹æ£€æµ‹æ—¶æ‰“å°
      if (enableDebugLog && (result.text.isNotEmpty || isEndpointDetected)) {
        // ignore: avoid_print
        print('[Pipeline] è¯†åˆ«: "${result.text}", endpoint: $isEndpointDetected');
      }
      if (isEndpointDetected && !_vadTriggeredStop) {
        await _handleEndpoint();
      }
    }
  }

  /// Story 3-7: å¤„ç†è®¾å¤‡ä¸¢å¤±äº‹ä»¶
  /// å½“ PortAudio æ£€æµ‹åˆ°è®¾å¤‡ä¸å¯ç”¨æ—¶è°ƒç”¨
  /// å‘é€å¸¦æœ‰ isDeviceLost=true çš„ EndpointEventï¼Œä¿å­˜å½“å‰è¯†åˆ«çš„æ–‡æœ¬
  void _handleDeviceLost() {
    if (_isDisposed || _endpointController.isClosed) return;

    // 1. å°è¯•è·å–å½“å‰å·²è¯†åˆ«çš„æ–‡æœ¬
    String preservedText = _lastEmittedText;

    // å°è¯•ä» ASREngine è·å–æœ€æ–°ç»“æœ
    try {
      _asrEngine.inputFinished();
      while (_asrEngine.isReady()) {
        _asrEngine.decode();
      }
      final result = _asrEngine.getResult();
      if (result.text.isNotEmpty) {
        preservedText = result.text;
      }
    } catch (e) {
      // å¿½ç•¥è§£ç é”™è¯¯ï¼Œä½¿ç”¨å·²æœ‰çš„æ–‡æœ¬
      if (enableDebugLog) {
        // ignore: avoid_print
        print('[Pipeline] âš ï¸ _handleDeviceLost è·å–æ–‡æœ¬å¤±è´¥: $e');
      }
    }

    // 2. è®¡ç®—å½•éŸ³æ—¶é•¿
    final durationMs = _recordingStartTime != null
        ? DateTime.now().difference(_recordingStartTime!).inMilliseconds
        : 0;

    // 3. å‘é€è®¾å¤‡ä¸¢å¤±äº‹ä»¶ (AC13: ä¿å­˜å·²è¯†åˆ«æ–‡æœ¬)
    final event = EndpointEvent(
      finalText: preservedText,
      isVadTriggered: false,
      durationMs: durationMs,
      latencyStats: latencyStats,
      isDeviceLost: true,
    );
    _endpointController.add(event);

    if (enableDebugLog) {
      // ignore: avoid_print
      print('[Pipeline] ğŸ”Œ è®¾å¤‡ä¸¢å¤±ï¼Œå·²ä¿å­˜æ–‡æœ¬: "$preservedText"');
    }
  }

  /// Story 2-6: å¤„ç† VAD ç«¯ç‚¹æ£€æµ‹
  Future<void> _handleEndpoint() async {
    try {
      // PTT ç´¯ç§¯æ¨¡å¼ï¼šä¸è°ƒç”¨ inputFinished()ï¼Œåªè·å–å½“å‰ç»“æœ
      // inputFinished() ä¼šä½¿å¼•æ“è¿›å…¥"ç»“æŸ"çŠ¶æ€ï¼Œå¯¼è‡´åç»­éŸ³é¢‘æ— æ³•ç´¯ç§¯
      final isPttAccumulateMode =
          !_vadConfig.autoStopOnEndpoint && !_vadConfig.autoReset;

      if (!isPttAccumulateMode) {
        // é PTT ç´¯ç§¯æ¨¡å¼ï¼šè°ƒç”¨ inputFinished() ç¡®ä¿æœ€ç»ˆè§£ç 
        _asrEngine.inputFinished();
        while (_asrEngine.isReady()) {
          _asrEngine.decode();
        }
      }

      final finalResult = _asrEngine.getResult();

      // 2. è®¡ç®—å½•éŸ³æ—¶é•¿
      final durationMs = _recordingStartTime != null
          ? DateTime.now().difference(_recordingStartTime!).inMilliseconds
          : 0;

      // 3. åˆ›å»ºå¹¶å‘é€ç«¯ç‚¹äº‹ä»¶
      final event = EndpointEvent(
        finalText: finalResult.text,
        isVadTriggered: true,
        durationMs: durationMs,
        latencyStats: latencyStats,
      );
      if (!_isDisposed && !_endpointController.isClosed) {
        _endpointController.add(event);
      }

      if (enableDebugLog) {
        // ignore: avoid_print
        print('[Pipeline] ğŸ¯ VAD ç«¯ç‚¹ (PTTç´¯ç§¯=$isPttAccumulateMode): "${finalResult.text}"');
      }

      // 4. æ ¹æ®é…ç½®å†³å®šåç»­è¡Œä¸º
      if (_vadConfig.autoStopOnEndpoint) {
        _vadTriggeredStop = true; // æ ‡è®° VAD è§¦å‘ï¼Œé˜²æ­¢ stop() é‡å¤å‘é€äº‹ä»¶
        _stopRequested = true;
      } else if (_vadConfig.autoReset) {
        _asrEngine.reset();
        _lastEmittedText = '';
        _recordingStartTime = DateTime.now();
      }
      // PTT ç´¯ç§¯æ¨¡å¼ (!autoStopOnEndpoint && !autoReset)ï¼šä»€ä¹ˆéƒ½ä¸åšï¼Œç»§ç»­ç´¯ç§¯
    } catch (e) {
      // FFI å±‚æˆ–å…¶ä»–å¼‚å¸¸å¤„ç†
      if (enableDebugLog) {
        // ignore: avoid_print
        print('[Pipeline] âš ï¸ _handleEndpoint å¼‚å¸¸: $e');
      }
      // å³ä½¿å‡ºé”™ä¹Ÿå°è¯•åœæ­¢ï¼Œé¿å…ç”¨æˆ·æ— æ„ŸçŸ¥
      _stopRequested = true;
    }
  }

  /// æ›´æ–°çŠ¶æ€å¹¶å‘é€äº‹ä»¶
  void _setState(PipelineState newState) {
    if (_state != newState) {
      _state = newState;
      // M1 ä¿®å¤: æ£€æŸ¥ StreamController æ˜¯å¦å·²å…³é—­
      if (!_isDisposed && !_stateController.isClosed) {
        _stateController.add(newState);
      }
    }
  }

  /// è®¾ç½®é”™è¯¯çŠ¶æ€
  void _setError(PipelineError error) {
    _lastError = error;
    _setState(PipelineState.error);
  }
}
